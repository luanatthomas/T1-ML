{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_leaf=1, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y)\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        num_samples, num_features = X.shape\n",
    "        unique_classes = np.unique(y)\n",
    "\n",
    "        # Se não há mais amostras ou atingimos a profundidade máxima, retorna a classe mais comum\n",
    "        if num_samples == 0 or (self.max_depth is not None and depth == self.max_depth):\n",
    "            return self._most_common_class(y)\n",
    "\n",
    "        # Se todas as amostras pertencem à mesma classe, retorna essa classe\n",
    "        if len(unique_classes) == 1:\n",
    "            return unique_classes[0]\n",
    "\n",
    "        # Se o número de amostras é menor que o mínimo para divisão, retorna a classe mais comum\n",
    "        if num_samples < self.min_samples_split:\n",
    "            return self._most_common_class(y)\n",
    "\n",
    "        # Encontrar a melhor divisão\n",
    "        best_feature, best_threshold = self._best_split(X, y)\n",
    "\n",
    "        # Se não houver divisão válida, retorna a classe mais comum\n",
    "        if best_feature is None:\n",
    "            return self._most_common_class(y)\n",
    "\n",
    "        # Dividir os dados\n",
    "        left_indices = X[:, best_feature] <= best_threshold\n",
    "        right_indices = X[:, best_feature] > best_threshold\n",
    "\n",
    "        # Verifica se os grupos têm amostras suficientes\n",
    "        if np.sum(left_indices) < self.min_samples_leaf or np.sum(right_indices) < self.min_samples_leaf:\n",
    "            return self._most_common_class(y)\n",
    "\n",
    "        left_tree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_tree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "        return (best_feature, best_threshold, left_tree, right_tree)\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        best_gain = -1\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "\n",
    "        for feature in range(num_features):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                gain = self._information_gain(X, y, feature, threshold)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _information_gain(self, X, y, feature, threshold):\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        left_indices = X[:, feature] <= threshold\n",
    "        right_indices = X[:, feature] > threshold\n",
    "\n",
    "        if np.sum(left_indices) == 0 or np.sum(right_indices) == 0:\n",
    "            return 0\n",
    "\n",
    "        left_entropy = self._entropy(y[left_indices])\n",
    "        right_entropy = self._entropy(y[right_indices])\n",
    "\n",
    "        # Calculando a entropia ponderada\n",
    "        n = len(y)\n",
    "        n_left = np.sum(left_indices)\n",
    "        n_right = np.sum(right_indices)\n",
    "\n",
    "        child_entropy = (n_left / n) * left_entropy + (n_right / n) * right_entropy\n",
    "        gain = parent_entropy - child_entropy\n",
    "\n",
    "        return gain\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        class_labels, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        return -np.sum(probabilities * np.log2(probabilities + 1e-10))\n",
    "\n",
    "    def _most_common_class(self, y):\n",
    "        return np.bincount(y).argmax()\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict(sample, self.tree) for sample in X])\n",
    "\n",
    "    def _predict(self, sample, tree):\n",
    "        if not isinstance(tree, tuple):\n",
    "            return tree\n",
    "\n",
    "        feature, threshold, left_tree, right_tree = tree\n",
    "        if sample[feature] <= threshold:\n",
    "            return self._predict(sample, left_tree)\n",
    "        else:\n",
    "            return self._predict(sample, right_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusão:\n",
      "[[50  0  0]\n",
      " [ 0 47  3]\n",
      " [ 0  1 49]]\n",
      "Acurácia: 0.97\n",
      "Precisão: 0.97\n",
      "Recall: 0.97\n",
      "F1-Score: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de uso com o dataset Iris\n",
    "if __name__ == \"__main__\":\n",
    "    iris = load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "\n",
    "    # Configuração dos parâmetros\n",
    "    tree = DecisionTree(max_depth=3, min_samples_leaf=2, min_samples_split=2)\n",
    "    tree.fit(X, y)\n",
    "\n",
    "    # Predições\n",
    "    predictions = tree.predict(X)\n",
    "\n",
    "    # Métricas de avaliação\n",
    "    cm = confusion_matrix(y, predictions)\n",
    "    accuracy = accuracy_score(y, predictions)\n",
    "    precision = precision_score(y, predictions, average='macro')\n",
    "    recall = recall_score(y, predictions, average='macro')\n",
    "    f1 = f1_score(y, predictions, average='macro')\n",
    "\n",
    "    # Resultados\n",
    "    print(f\"Matriz de Confusão:\\n{cm}\")\n",
    "    print(f\"Acurácia: {accuracy:.2f}\")\n",
    "    print(f\"Precisão: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1-Score: {f1:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
